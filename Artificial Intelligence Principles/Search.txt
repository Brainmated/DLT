Problem solving agents are goal-directed agents:
  1. Goal Formulation: Set of one or more (desirable) world states (e.g. checkmate in chess)
  2. Problem Formulation: What actions and states to consider given a goal and initial state
  3. Search for a solution: Given the problem, search for a solution -- a sequence of actions to achieve the goal starting from the initial state
  4. Execution of solution

Example: Path Finding Problem

1. Formulate goal, get to X city
2. Formulate Problem, drive between cities starting from an initial state
3. Find solution, try sequences of cities, determine efficiency with each different path
4. Execute Solution, drive from initial state to X city according to the best solution

Problem Types ~
  1. Deterministic, fully observable: Agent knows exactly which state it will be in; the solution is a sequence of actions
  2. Non-observable, sensorless problem: Agent may not know where it is due to the lack of sensors; it reasons in terms of belief states; solution is still a sequence of actions
  3. Nondeterministic and/or partially observable: contingency problem: Actions are uncertain; percepts provide new information about current state; The Solution is a "strategy" to reach the goal.
  4. Unknown state space and uncertain action effects: exploration problem; The Solution is a "strategy" to reach the goal and to explore the environment.

Search Strategies ~
Strategies are evaluated along hte following dimensions:
  1. Completeness: does it always find a solution if one exists?
  2. Time Complexity: number of nodes generated
  3. Space Complexity: maximum number of nodes in memory
  4. Optimality: does it always find the least-cost solution?

Uninformed search strategies ~
  -Breadth-first search
  -Uniform-cost search
  -Depth-first search
  -Depth-limited search
  -Iterative search
  -Bidirectional search
